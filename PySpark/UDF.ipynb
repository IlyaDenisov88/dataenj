{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7Q+Ico9mbEn1uBL/W3Zw1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyaDenisov88/dataenj/blob/main/PySpark/UDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyTmiYojalKP",
        "outputId": "f164652b-546f-490d-d1cd-a6e587807728"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=4ef3d918cc0438744632ca1626921ff254463099e655676751feeee97b1c6186\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UDF (User-Defined Function)** в Apache Spark — это пользовательская функция, которая позволяет выполнять произвольные вычисления над данными в DataFrame.\n",
        "\n",
        "- UDF можно использовать для применения сложной логики или вычислений, которые не поддерживаются стандартными функциями Spark SQL.\n",
        "- В целом, создание UDF очень простое. Необходимо всего лишь импортнуть библиотеку udf. Рассмотрим примеры -\n",
        "\n"
      ],
      "metadata": {
        "id": "yg-usjtTaEBd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "No2ThfzPZ_9p",
        "outputId": "d7b51e96-77d1-44ab-bb9f-ca59bbfd0b10"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-------------+\n",
            "| id| name|prefixed_name|\n",
            "+---+-----+-------------+\n",
            "|  1|Alice|   Name_Alice|\n",
            "|  2|  Bob|     Name_Bob|\n",
            "|  3|Cathy|   Name_Cathy|\n",
            "|  4|David|   Name_David|\n",
            "+---+-----+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import IntegerType, StringType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"UDF Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\"), (4, \"David\")]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
        "\n",
        "# Определяем пользовательскую функцию\n",
        "def add_prefix(name):\n",
        "    return \"Name_\" + name\n",
        "\n",
        "# Регистрируем функцию как UDF\n",
        "add_prefix_udf = udf(add_prefix, StringType())\n",
        "\n",
        "# Применяем UDF к DataFrame\n",
        "df_with_prefix = df.withColumn(\"prefixed_name\", add_prefix_udf(col(\"name\")))\n",
        "\n",
        "df_with_prefix.show()\n",
        "\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "В теле функции может быть все, что угодно. Но, если Вы хотите использовать UDF в рамках чистого SQL, то конструкция создания UDF будет немного другая.\n",
        "\n",
        "Метод `spark.udf.register` используется для регистрации пользовательских функций (UDF) в Spark, чтобы их можно было использовать в SQL-запросах. Рассмотрим пример.\n",
        "\n"
      ],
      "metadata": {
        "id": "6chR2dcya4xT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import IntegerType\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Register UDF Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "\n",
        "data = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Cathy\"), (4, \"David\")]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\"])\n",
        "\n",
        "\n",
        "def name_length(name):\n",
        "    return len(name)\n",
        "\n",
        "# Регистрируем функцию как UDF с использованием spark.udf.register\n",
        "spark.udf.register(\"name_length_udf\", name_length, IntegerType())\n",
        "\n",
        "# Создаем временную таблицу для выполнения SQL-запросов\n",
        "df.createOrReplaceTempView(\"people\")\n",
        "\n",
        "# Используем зарегистрированную UDF в SQL-запросе\n",
        "result_df = spark.sql(\"SELECT id, name, name_length_udf(name) as name_length FROM people\")\n",
        "\n",
        "\n",
        "result_df.show()\n",
        "\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gm1izXS5a9Sg",
        "outputId": "799f5536-9e64-4e98-da42-d8e4057116d3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----------+\n",
            "| id| name|name_length|\n",
            "+---+-----+-----------+\n",
            "|  1|Alice|          5|\n",
            "|  2|  Bob|          3|\n",
            "|  3|Cathy|          5|\n",
            "|  4|David|          5|\n",
            "+---+-----+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}