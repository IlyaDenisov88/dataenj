{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP+QkyCpAi22rZpTMO8TFDg"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psq5fQm7lXz2",
        "outputId": "0ebde3e3-6138-4125-c5a2-085286ec0717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pip 24.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n"
          ]
        }
      ],
      "source": [
        "pip --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark py4j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yarMw-gllq31",
        "outputId": "f7a13020-f95f-4487-bbc4-e8488dfeb5b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=9f5540089915a03e4f7d3066d6a02f544d12ee1eff055f3e5a187aae0f110a5a\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext, SparkConf\n",
        "\n",
        "# Настройка Spark\n",
        "\n",
        "conf = SparkConf().setAppName(\"Simple RDD Example\").setMaster(\"local[*]\")\n",
        "\n",
        "sc = SparkContext(conf=conf)\n",
        "\n",
        "# 1. Создание RDD из списка чисел\n",
        "\n",
        "numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "\n",
        "rdd = sc.parallelize(numbers)\n",
        "\n",
        "# 2. Трансформации: Фильтрация чётных чисел\n",
        "\n",
        "even_numbers_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "# 3. Действие: Подсчёт суммы чётных чисел\n",
        "\n",
        "sum_even_numbers = even_numbers_rdd.sum()\n",
        "\n",
        "# Вывод результата\n",
        "\n",
        "print(\"Чётные числа:\", even_numbers_rdd.collect())\n",
        "\n",
        "print(\"Сумма чётных чисел:\", sum_even_numbers)\n",
        "\n",
        "a = 5 + 6\n",
        "\n",
        "print(a)\n",
        "\n",
        "# Остановка SparkContext\n",
        "\n",
        "sc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rj4n0Rrvlxec",
        "outputId": "6746d53e-c322-40ce-de9f-42165dc4bd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Чётные числа: [2, 4, 6, 8, 10]\n",
            "Сумма чётных чисел: 30\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример создания Spark Session на Python:\n"
      ],
      "metadata": {
        "id": "Fwhh3OQYQQcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание объекта SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MySparkApp\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Здесь можно выполнять операции с DataFrames\n",
        "# Закрытие SparkSession\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "QkmGwst_QPhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример создания Spark Session с параметрами конфигурации:\n",
        "\n"
      ],
      "metadata": {
        "id": "4y7_hxiXWv3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание Spark Session с различными параметрами конфигурации\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"MyApp\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.executor.cores\", \"4\") \\\n",
        "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
        "    .config(\"spark.checkpoint.dir\", \"/path/to/checkpoint/dir\") \\\n",
        "    .config(\"spark.sql.warehouse.dir\", \"/path/to/warehouse/dir\") \\\n",
        "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
        "    .enableHiveSupport() \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Пример использования Spark Session\n",
        "df = spark.read.json(\"/path/to/json/file\")\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "collapsed": true,
        "id": "B_TD9RMfWvBM",
        "outputId": "a5c9183f-6e12-4249-f959-a32c2ec4f099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/path/to/json/file.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-47a52188af10>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Пример использования Spark Session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/json/file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers)\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/path/to/json/file."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Workflow приложения Spark\n"
      ],
      "metadata": {
        "id": "8lpQEhXNe89i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск автономного приложения и инициализация SparkContext\n",
        "\n"
      ],
      "metadata": {
        "id": "fvR94MgNe-bA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание SparkSession\n",
        "spark = SparkSession.builder.appName(\"Spark Workflow Example\").getOrCreate()\n",
        "\n",
        "# Получение SparkContext\n",
        "sc = spark.sparkContext"
      ],
      "metadata": {
        "id": "t7db1VUNe8yS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Приложение, написанное на Python, Scala, Java или R, запускается автономно.\n",
        "- В этом приложении создается объект SparkSession (который включает SparkContext), инициализирующий необходимые компоненты для работы Spark.\n",
        "- Только при наличии SparkContext приложение называется драйвером (Driver)\n"
      ],
      "metadata": {
        "id": "s1FBcwuFfDYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запрос ресурсов у менеджера кластеров\n",
        "\n"
      ],
      "metadata": {
        "id": "DUm1TVBufM8K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Драйвер запрашивает у кластерного менеджера (например, YARN, Mesos, Kubernetes или Spark Standalone) ресурсы для выполнения задачи.\n",
        "* Драйвер сообщает кластерному менеджеру, сколько ресурсов ему требуется (например, количество исполнителей, объем памяти и процессорные ядра).\n",
        "\n"
      ],
      "metadata": {
        "id": "hl5MHD2kfOXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark-submit \\\n",
        "  --master yarn \\\n",
        "  --deploy-mode cluster \\\n",
        "  --num-executors 10 \\\n",
        "  --executor-memory 4g \\\n",
        "  --executor-cores 2 \\\n",
        "  path/to/your_script.py"
      ],
      "metadata": {
        "id": "k3SDNQcefC35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск исполнителей менеджером кластеров\n",
        "\n"
      ],
      "metadata": {
        "id": "lh94CtN8fX_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Кластерный менеджер распределяет ресурсы и запускает процессы исполнителей (executors) на рабочих узлах (worker nodes).\n",
        "- Каждый исполнитель запускает отдельный процесс, который будет выполнять задачи, переданные драйвером."
      ],
      "metadata": {
        "id": "lFXjk9O5fX2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запуск кода Spark драйвером\n",
        "\n"
      ],
      "metadata": {
        "id": "1G-et6InfgTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   Драйвер начинает выполнение кода Spark, который включает чтение данных, выполнение преобразований (transformations) и действий (actions).\n",
        "* Драйвер разделяет задачи на этапы (stages) и назначает их исполнителям.\n",
        "\n"
      ],
      "metadata": {
        "id": "NdKiqGjwfhKL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных\n",
        "df = spark.read.csv(\"path/to/input.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Выполнение преобразований\n",
        "df_filtered = df.filter(df[\"age\"] > 30)\n",
        "df_grouped = df_filtered.groupBy(\"city\").count()"
      ],
      "metadata": {
        "id": "e2_ktPrNfl4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполнение заданий исполнителями и отправка результатов драйверу\n",
        "\n"
      ],
      "metadata": {
        "id": "0sjslqYYfgRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Исполнители получают задания от драйвера и начинают их выполнение.\n",
        "* Каждый исполнитель выполняет часть работы, такой как вычисления на данных и возвращает результаты драйверу.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L8IExx5pfqYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Действие, инициирующее выполнение заданий\n",
        "result = df_grouped.collect()"
      ],
      "metadata": {
        "id": "pLeON-TYfqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Остановка SparkContext и освобождение ресурсов\n",
        "\n"
      ],
      "metadata": {
        "id": "ACNVb7KQfgLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* После завершения выполнения всех задач, драйвер останавливает SparkContext.\n",
        "* Исполнители закрываются и освобождают выделенные им ресурсы, возвращая их обратно в кластер."
      ],
      "metadata": {
        "id": "GAC8WN49fziu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Остановка SparkSession и SparkContext\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "1QLDmYoFlxcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RDD vs Dataset vs Dataframe"
      ],
      "metadata": {
        "id": "lglGPuD4uB5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RDD (Resilient Distributed Dataset)"
      ],
      "metadata": {
        "id": "jLfA8bU4uGZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Схема данных:\n",
        "\n",
        "- В RDD схема данных неявно определяется структурой объектов, хранящихся в RDD.\n",
        "- RDD является типизированной коллекцией, где каждый элемент может быть объектом любого типа, но Spark не хранит информацию о структуре этих данных."
      ],
      "metadata": {
        "id": "fqqXUTomuXyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkContext\n",
        "\n",
        "sc = SparkContext(\"local\", \"RDD Example\")\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Схема данных неявно определяется структурой кортежей\n",
        "print(rdd.collect())  # [('Alice', 1), ('Bob', 2), ('Cathy', 3)]\n"
      ],
      "metadata": {
        "id": "6G0JavOJuFJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Методы rdd"
      ],
      "metadata": {
        "id": "nZygX7QQ0aWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from operator import add\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"MySparkApp\") \\\n",
        "        .config(\"spark.master\", \"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "data = [1,2,3,3,4,4,5,6,7,8,9,10]\n",
        "data2 = [1,2,3,3,40]\n",
        "rdd = sc.parallelize(data)\n",
        "rdd2 = sc.parallelize(data2)\n",
        "\n",
        "rdd_mapped = rdd.map(lambda x: x*2) # метод map к каждому элементу применяет функцию\n",
        "rdd_filtered = rdd.filter(lambda x: x%2 == 0) # метод filter отбирает элементы по условию\n",
        "rdd_flat_mapped = rdd.flatMap(lambda x: [x, x*2]) # добавляет удвоенный элемент после обычного\n",
        "rdd_dist = rdd.distinct() # выводит уникальные элементы\n",
        "rdd_union = rdd.union(rdd2) # соединение\n",
        "print(rdd.count()) # длина rdd\n",
        "print(rdd.first())\n",
        "print(rdd2.take(2)) # берет 2 элемента слева [:2]\n",
        "print(\"reduce\", rdd2.reduce(add)) # складываем все элементы используя модуль operator\n",
        "def print_square(x):\n",
        "  square = x*x\n",
        "  print(f\"Square of {x} is {square}\")\n",
        "\n",
        "rdd2.foreach(print_square) # метод foreach работает только если мы работаем на кластере (а не на local)\n",
        "# локально метод foreach не работает\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVa-2bzVvX00",
        "outputId": "7e850bc8-a1f7-4326-af1d-750b71b5656b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "1\n",
            "[1, 2]\n",
            "reduce 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame"
      ],
      "metadata": {
        "id": "h3qt5enIuIpu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Схема данных:\n",
        "\n",
        "- В DataFrame схема данных явная и включает имена столбцов и типы данных.\n",
        "- Схема может быть автоматически определена при создании DataFrame из источника данных (например, CSV, JSON) или явно задана при создании DataFrame."
      ],
      "metadata": {
        "id": "btpJlUT8ub_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataFrame Example\").getOrCreate()\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Cathy\", 3)]\n",
        "\n",
        "# Явное определение схемы\n",
        "schema = StructType([\n",
        "    StructField(\"Name\", StringType(), True),\n",
        "    StructField(\"Value\", IntegerType(), True)\n",
        "])\n",
        "\n",
        "# Создание DataFrame с явной схемой\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.printSchema()\n",
        "\n",
        "# Автоматическое определение схемы при чтении данных из CSV\n",
        "df_auto = spark.read.csv(\"/path/to/csv/file\", header=True, inferSchema=True)\n",
        "df_auto.printSchema()"
      ],
      "metadata": {
        "id": "Y3YdygCOuIbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Схема в DataFrame"
      ],
      "metadata": {
        "id": "iUB4Fc9I0i0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"schema\") \\\n",
        ".config(\"spark.master\", \"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 25), (\"Artem\", 26), (\"Ivan\", 48)]\n",
        "\n",
        "# Схема - название и типы полей в таблице\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "])# последний параметр true - говорит может ли быть null значение (nullable = true)\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.show()\n",
        "\n",
        "spark.stop()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfy_DP630m6Z",
        "outputId": "f0b6fc9f-4626-448a-b13c-7fb827e3d645"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|Artem| 26|\n",
            "| Ivan| 48|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Автоматическое создание схемы"
      ],
      "metadata": {
        "id": "tB1KMTX922u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"schema\") \\\n",
        ".config(\"spark.master\", \"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "data = [{\"name\":\"Alice\", \"age\":29},\n",
        "        {\"name\":\"Artem\", \"age\":120},\n",
        "        {\"name\":\"Ivan\", \"age\":56}]\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.show()\n",
        "\n",
        "spark.stop()\n",
        "\n",
        "# тип age стал long - не очень подходит, но в принципе данный способ хорош"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGdZGU-m1Kkp",
        "outputId": "bdfa348d-9f93-4037-f85c-15c2c3fa38e9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- age: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            "\n",
            "+---+-----+\n",
            "|age| name|\n",
            "+---+-----+\n",
            "| 29|Alice|\n",
            "|120|Artem|\n",
            "| 56| Ivan|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создание датафрейма из RDD и методы работы с датафреймами"
      ],
      "metadata": {
        "id": "AGBkZ9Jf3mIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import avg, max\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"schema\") \\\n",
        ".config(\"spark.master\", \"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 29), (\"Bob\", 87)]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data) # можно делать сразу\n",
        "\n",
        "df = rdd.toDF([\"name\", \"age\"])# если не указать, то названия столбцов будут созданы автоматически\n",
        "\n",
        "df.printSchema()\n",
        "\n",
        "df.show(1) #truncate=False - показывает таблицу полностью\n",
        "\n",
        "print(df.head(1)) # берет шапку - не выводит\n",
        "\n",
        "print(df.take(3)) # берет шапку в виде списка - не выводит\n",
        "\n",
        "columns = df.columns\n",
        "print(columns) # returns list\n",
        "\n",
        "columns = df.dtypes\n",
        "print(columns) # returns list of tuples ('column', 'dtype')\n",
        "\n",
        "# работают здесь и методы SQL\n",
        "df.select(\"name\").show()\n",
        "df.selectExpr(\"name\", \"age+1 as age_plus_one\").show()\n",
        "\n",
        "df.filter(df[\"age\"] > 30).show() # то же самое с методом where\n",
        "\n",
        "df.groupBy(\"age\").count().show()# группировка\n",
        "df.groupBy(\"age\").agg(\n",
        "    avg(\"age\").alias(\"average_age\"),\n",
        "    max(\"age\").alias(\"max_age\")\n",
        ").show() # агрегация\n",
        "\n",
        "df.orderBy(\"age\").show() # df.sort(\"age\", ascending=True)\n",
        "df.orderBy(df[\"age\"].desc()).show()# df.sort(\"age\", ascending=False)\n",
        "\n",
        "df.withColumn(\"age_plus_one\", df[\"age\"] + 1).show() # добавление столбца\n",
        "\n",
        "df.withColumnRenamed(\"age\", \"fio\").show() # переименовать столбец\n",
        "\n",
        "df.drop(\"age_plus_one\").show() # удаление столбца\n",
        "\n",
        "df.distinct().show() # отобразить без строк-дубликатов\n",
        "\n",
        "df.limit(1).show() # 1 первую строку\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UIMn6gq73Q55",
        "outputId": "aeb5f148-9a9f-4403-dbc7-05089df49bec"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: long (nullable = true)\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 29|\n",
            "+-----+---+\n",
            "only showing top 1 row\n",
            "\n",
            "[Row(name='Alice', age=29)]\n",
            "[Row(name='Alice', age=29), Row(name='Bob', age=87)]\n",
            "['name', 'age']\n",
            "[('name', 'string'), ('age', 'bigint')]\n",
            "+-----+\n",
            "| name|\n",
            "+-----+\n",
            "|Alice|\n",
            "|  Bob|\n",
            "+-----+\n",
            "\n",
            "+-----+------------+\n",
            "| name|age_plus_one|\n",
            "+-----+------------+\n",
            "|Alice|          30|\n",
            "|  Bob|          88|\n",
            "+-----+------------+\n",
            "\n",
            "+----+---+\n",
            "|name|age|\n",
            "+----+---+\n",
            "| Bob| 87|\n",
            "+----+---+\n",
            "\n",
            "+---+-----+\n",
            "|age|count|\n",
            "+---+-----+\n",
            "| 29|    1|\n",
            "| 87|    1|\n",
            "+---+-----+\n",
            "\n",
            "+---+-----------+-------+\n",
            "|age|average_age|max_age|\n",
            "+---+-----------+-------+\n",
            "| 29|       29.0|     29|\n",
            "| 87|       87.0|     87|\n",
            "+---+-----------+-------+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 29|\n",
            "|  Bob| 87|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|  Bob| 87|\n",
            "|Alice| 29|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+------------+\n",
            "| name|age|age_plus_one|\n",
            "+-----+---+------------+\n",
            "|Alice| 29|          30|\n",
            "|  Bob| 87|          88|\n",
            "+-----+---+------------+\n",
            "\n",
            "+-----+---+\n",
            "| name|fio|\n",
            "+-----+---+\n",
            "|Alice| 29|\n",
            "|  Bob| 87|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 29|\n",
            "|  Bob| 87|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 29|\n",
            "|  Bob| 87|\n",
            "+-----+---+\n",
            "\n",
            "+-----+---+\n",
            "| name|age|\n",
            "+-----+---+\n",
            "|Alice| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "from pyspark.sql.functions import avg, max\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"schema\") \\\n",
        ".config(\"spark.master\", \"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "df1 = spark.createDataFrame([(1, \"artem\"), (2, \"bob\"), (3, \"ivan\")], [\"id\", \"name\"])\n",
        "df2 = spark.createDataFrame([(1, 100), (2, 200)], [\"id\", \"money\"])\n",
        "\n",
        "df_join = df1.join(df2, \"id\", \"left\")\n",
        "df_join.show()\n",
        "\n",
        "df1.sample(withReplacement=False, fraction=0.5).show() #случайная выборка данных из датафрейма(50%)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sryb-_zV91ZB",
        "outputId": "865aa775-4362-4703-a093-e6e5c1c59b2f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+-----+\n",
            "| id| name|money|\n",
            "+---+-----+-----+\n",
            "|  1|artem|  100|\n",
            "|  3| ivan| NULL|\n",
            "|  2|  bob|  200|\n",
            "+---+-----+-----+\n",
            "\n",
            "+---+-----+\n",
            "| id| name|\n",
            "+---+-----+\n",
            "|  1|artem|\n",
            "+---+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset (доступен только в Scala и Java)\n"
      ],
      "metadata": {
        "id": "IUSbK5VHuNYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Схема данных:\n",
        "\n",
        "- В Dataset схема данных явная и определяется структурой типизированных объектов (классов), хранящихся в Dataset.\n",
        "- Dataset сочетает в себе преимущества RDD (типизированный API) и DataFrame (явная схема и оптимизация)."
      ],
      "metadata": {
        "id": "KrXTMB1vugpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import org.apache.spark.sql.SparkSession\n",
        "import org.apache.spark.sql.types._\n",
        "\n",
        "val spark = SparkSession.builder.appName(\"Dataset Example\").getOrCreate()\n",
        "import spark.implicits._\n",
        "\n",
        "case class Person(name: String, age: Int)\n",
        "val data = Seq(Person(\"Alice\", 1), Person(\"Bob\", 2), Person(\"Cathy\", 3))\n",
        "val ds = data.toDS()\n",
        "\n",
        "// Схема данных определяется структурой класса Person\n",
        "ds.printSchema()"
      ],
      "metadata": {
        "id": "Jg17TVpHuPJC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}