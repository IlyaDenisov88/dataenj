{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMdnrWbsZVHPnXzMdVjR2XN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyaDenisov88/dataenj/blob/main/PySpark/Spark_submit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Spark-submit"
      ],
      "metadata": {
        "id": "03LAvk9gBIEG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp_PCrFgARHS",
        "outputId": "d27ac6fd-4557-4402-876a-38a5173be4f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=bbfda2ae5300c9e768ccabf880a0cb89bfa053aab1d3c28bcc9ab9e833f14d84\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CuOi-qUpAPs8"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Tungsten Example\") \\\n",
        "    .config(\"spark.sql.codegen.wholeStage\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "data = [(i, f\"Name_{i % 5}\", i % 3, i % 2) for i in range(1, 1000001)]\n",
        "df = spark.createDataFrame(data, [\"id\", \"name\", \"mod3\", \"mod2\"])\n",
        "\n",
        "# Применяем фильтр и агрегацию, чтобы увидеть работу Tungsten\n",
        "result = df.filter(col(\"mod2\") == 1) \\\n",
        "           .groupBy(\"mod3\") \\\n",
        "           .agg({\"id\": \"sum\"}) \\\n",
        "           .orderBy(\"mod3\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --master local\\[4\\] \\\n",
        "             --name \"Tungsten Example\" \\\n",
        "             --conf spark.sql.codegen.wholeStage=true \\\n",
        "             --conf spark.executor.memory=2g \\\n",
        "             main.py# здесь сохранен текст пред ячейки"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gpnqvKGPAdsk",
        "outputId": "fbec1f4b-20af-4442-af95-6cf9e92649e5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Missing application resource.\n",
            "\n",
            "Usage: spark-submit [options] <app jar | python file | R file> [app arguments]\n",
            "Usage: spark-submit --kill [submission ID] --master [spark://...]\n",
            "Usage: spark-submit --status [submission ID] --master [spark://...]\n",
            "Usage: spark-submit run-example [options] example-class [example args]\n",
            "\n",
            "Options:\n",
            "  --master MASTER_URL         spark://host:port, mesos://host:port, yarn,\n",
            "                              k8s://https://host:port, or local (Default: local[*]).\n",
            "  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (\"client\") or\n",
            "                              on one of the worker machines inside the cluster (\"cluster\")\n",
            "                              (Default: client).\n",
            "  --class CLASS_NAME          Your application's main class (for Java / Scala apps).\n",
            "  --name NAME                 A name of your application.\n",
            "  --jars JARS                 Comma-separated list of jars to include on the driver\n",
            "                              and executor classpaths.\n",
            "  --packages                  Comma-separated list of maven coordinates of jars to include\n",
            "                              on the driver and executor classpaths. Will search the local\n",
            "                              maven repo, then maven central and any additional remote\n",
            "                              repositories given by --repositories. The format for the\n",
            "                              coordinates should be groupId:artifactId:version.\n",
            "  --exclude-packages          Comma-separated list of groupId:artifactId, to exclude while\n",
            "                              resolving the dependencies provided in --packages to avoid\n",
            "                              dependency conflicts.\n",
            "  --repositories              Comma-separated list of additional remote repositories to\n",
            "                              search for the maven coordinates given with --packages.\n",
            "  --py-files PY_FILES         Comma-separated list of .zip, .egg, or .py files to place\n",
            "                              on the PYTHONPATH for Python apps.\n",
            "  --files FILES               Comma-separated list of files to be placed in the working\n",
            "                              directory of each executor. File paths of these files\n",
            "                              in executors can be accessed via SparkFiles.get(fileName).\n",
            "  --archives ARCHIVES         Comma-separated list of archives to be extracted into the\n",
            "                              working directory of each executor.\n",
            "\n",
            "  --conf, -c PROP=VALUE       Arbitrary Spark configuration property.\n",
            "  --properties-file FILE      Path to a file from which to load extra properties. If not\n",
            "                              specified, this will look for conf/spark-defaults.conf.\n",
            "\n",
            "  --driver-memory MEM         Memory for driver (e.g. 1000M, 2G) (Default: 1024M).\n",
            "  --driver-java-options       Extra Java options to pass to the driver.\n",
            "  --driver-library-path       Extra library path entries to pass to the driver.\n",
            "  --driver-class-path         Extra class path entries to pass to the driver. Note that\n",
            "                              jars added with --jars are automatically included in the\n",
            "                              classpath.\n",
            "\n",
            "  --executor-memory MEM       Memory per executor (e.g. 1000M, 2G) (Default: 1G).\n",
            "\n",
            "  --proxy-user NAME           User to impersonate when submitting the application.\n",
            "                              This argument does not work with --principal / --keytab.\n",
            "\n",
            "  --help, -h                  Show this help message and exit.\n",
            "  --verbose, -v               Print additional debug output.\n",
            "  --version,                  Print the version of current Spark.\n",
            "\n",
            " Spark Connect only:\n",
            "   --remote CONNECT_URL       URL to connect to the server for Spark Connect, e.g.,\n",
            "                              sc://host:port. --master and --deploy-mode cannot be set\n",
            "                              together with this option. This option is experimental, and\n",
            "                              might change between minor releases.\n",
            "\n",
            " Cluster deploy mode only:\n",
            "  --driver-cores NUM          Number of cores used by the driver, only in cluster mode\n",
            "                              (Default: 1).\n",
            "\n",
            " Spark standalone or Mesos with cluster deploy mode only:\n",
            "  --supervise                 If given, restarts the driver on failure.\n",
            "\n",
            " Spark standalone, Mesos or K8s with cluster deploy mode only:\n",
            "  --kill SUBMISSION_ID        If given, kills the driver specified.\n",
            "  --status SUBMISSION_ID      If given, requests the status of the driver specified.\n",
            "\n",
            " Spark standalone and Mesos only:\n",
            "  --total-executor-cores NUM  Total cores for all executors.\n",
            "\n",
            " Spark standalone, YARN and Kubernetes only:\n",
            "  --executor-cores NUM        Number of cores used by each executor. (Default: 1 in\n",
            "                              YARN and K8S modes, or all available cores on the worker\n",
            "                              in standalone mode).\n",
            "\n",
            " Spark on YARN and Kubernetes only:\n",
            "  --num-executors NUM         Number of executors to launch (Default: 2).\n",
            "                              If dynamic allocation is enabled, the initial number of\n",
            "                              executors will be at least NUM.\n",
            "  --principal PRINCIPAL       Principal to be used to login to KDC.\n",
            "  --keytab KEYTAB             The full path to the file that contains the keytab for the\n",
            "                              principal specified above.\n",
            "\n",
            " Spark on YARN only:\n",
            "  --queue QUEUE_NAME          The YARN queue to submit to (Default: \"default\").\n",
            "      \n"
          ]
        }
      ]
    }
  ]
}