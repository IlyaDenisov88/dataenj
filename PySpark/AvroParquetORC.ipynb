{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyAgQEAGY6Mkgd+cVcqCvv"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuQK35mD8_mP",
        "outputId": "711637c3-0bf8-4dba-9469-3bcaa2dd218f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement 2.4.3 (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for 2.4.3\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install pyspark 2.4.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install avro"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fP8nZJ8-eqT",
        "outputId": "30b8ec3b-c0c1-479a-decf-ede3aeff012e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: avro in /usr/local/lib/python3.10/dist-packages (1.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Запись данных в форматы Avro, Parquet и ORC"
      ],
      "metadata": {
        "id": "4sujs0r-9q3u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись данных в Avro"
      ],
      "metadata": {
        "id": "6dyx6-XG9LrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.avro.functions import from_avro, to_avro"
      ],
      "metadata": {
        "id": "P9aBDcFh_Erp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание SparkSession\n",
        "spark = SparkSession.builder.appName(\"Write Avro Example\").getOrCreate()\n",
        "\n",
        "# Пример данных\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Запись данных в Avro\n",
        "df.write.to_avro.save(\"avro\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "nwN06izS9Ix5",
        "outputId": "5b2853a4-9183-4dd8-8095-e7565ab19f96"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrameWriter' object has no attribute 'to_avro'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-63fbe17c5d5c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Запись данных в Avro\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_avro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"avro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrameWriter' object has no attribute 'to_avro'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись данных в Parquet\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "srKaMylX9OP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание SparkSession\n",
        "spark = SparkSession.builder.appName(\"Write Parquet Example\").getOrCreate()\n",
        "\n",
        "# Пример данных\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Запись данных в Parquet\n",
        "df.write.parquet(\"parquet\")"
      ],
      "metadata": {
        "id": "qYSyvsHD9Nvh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделение на разные паркет файлы по Age"
      ],
      "metadata": {
        "id": "pq3TC_FjCp4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"Save\") \\\n",
        ".config(\"spark.master\", \"local[*]\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "data = [(\"Alice\", 29), (\"Bob\", 25), (\"Charlie\", 32)]\n",
        "columns = [\"Name\", \"Age\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "df.write.partitionBy(\"Age\").parquet(\"time\", mode=\"overwrite\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "WsaQQmR8BX4E"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запись данных в ORC\n"
      ],
      "metadata": {
        "id": "DLu2YDkq9U9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Создание SparkSession\n",
        "spark = SparkSession.builder.appName(\"Write ORC Example\").getOrCreate()\n",
        "\n",
        "# Пример данных\n",
        "data = [(\"Alice\", 25), (\"Bob\", 30), (\"Cathy\", 29)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# Запись данных в ORC\n",
        "df.write.orc(\"orc\")"
      ],
      "metadata": {
        "id": "h-5ul_VI9UEJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Чтение данных из форматов Avro, Parquet и ORC"
      ],
      "metadata": {
        "id": "UBTJatRw9Y9X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение данных из Avro"
      ],
      "metadata": {
        "id": "Zuh4PBl_9bCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из Avro\n",
        "df_avro = spark.read.format(\"avro\").load(\"avro\")\n",
        "df_avro.show()"
      ],
      "metadata": {
        "id": "cgY4bHcH9gD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Чтение данных из Parquet"
      ],
      "metadata": {
        "id": "afSDu7JD9gfo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из Parquet\n",
        "df_parquet = spark.read.parquet(\"parquet\")\n",
        "df_parquet.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55N8Lef_9kF8",
        "outputId": "19f6d73e-0994-4abf-a0db-1ff281e1cc1c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтение данных из ORC"
      ],
      "metadata": {
        "id": "7oUtKSzT9koB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Чтение данных из ORC\n",
        "df_orc = spark.read.orc(\"orc\")\n",
        "df_orc.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZMhYoIH9YSF",
        "outputId": "0cde9020-0e91-431c-b61d-de386161159a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+\n",
            "| Name|Age|\n",
            "+-----+---+\n",
            "|Alice| 25|\n",
            "|  Bob| 30|\n",
            "|Cathy| 29|\n",
            "+-----+---+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}