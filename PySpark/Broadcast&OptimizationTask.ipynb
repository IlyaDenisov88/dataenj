{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM2CHIUcibnsDrOLdB8NzTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyaDenisov88/dataenj/blob/main/PySpark/Broadcast%26OptimizationTask.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Broadcast"
      ],
      "metadata": {
        "id": "rknpCFK6gz8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Давайте также рассмотрим, что такое функция `broadcast` и что она делает.\n",
        "\n",
        "- Broadcast переменные используются для эффективного распределения больших объектов или небольших наборов данных по всем узлам кластера.\n",
        "- позволяет избежать многократной передачи одних и тех же данных при выполнении распределенных вычислений.\n",
        "- Broadcast переменные полезны, когда у вас есть неизменяемые данные, которые нужно использовать в нескольких местах в вашем приложении Spark, и эти данные достаточно велики, чтобы их передача в каждый узел была неэффективной.\n",
        "\n",
        "Ничего не понятно, но очень интересно... Давайте разбираться.\n",
        "\n",
        "Я предвкушаю Ваши вопросы по поводу того, а в чем разница в сравнении с `cache` и с `persist` соответственно? В ходе объяснения постараюсь рассказать Вам эту разницу.\n",
        "\n",
        "- Broadcast чаще всего используется в случае, когда у Вас есть какой-то справочник с данными. То есть он не будет уже изменяться. И, поскольку, он хранится на всех узлах одновременно, то у нас нет необходимости каждый раз вызывать этап `shuffle` для перемещения ключей. Приведу простой пример, когда это может использоваться.\n",
        "\n",
        "Например, если у вас есть большой DataFrame и маленький DataFrame, и вы хотите выполнить join, оптимальным решением будет использовать broadcast переменную для маленького DataFrame.\n",
        " - Это позволяет *избежать дорогих shuffle операций*, так как маленький DataFrame будет распространяться по всем узлам и join будет выполняться локально на каждом узле.\n",
        "\n"
      ],
      "metadata": {
        "id": "qjDydQ1pfnfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь взглянем на код -\n",
        "\n"
      ],
      "metadata": {
        "id": "Jc1vyTSfgP1e"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNQTT0J9eLN2"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Broadcast Join Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Пример большого DataFrame\n",
        "large_data = [(i, f\"Name_{i % 5}\") for i in range(1000000)]\n",
        "large_df = spark.createDataFrame(large_data, [\"id\", \"name\"])\n",
        "\n",
        "# Пример маленького DataFrame\n",
        "small_data = [(1, \"HR\"), (2, \"Finance\"), (3, \"Engineering\")]\n",
        "small_df = spark.createDataFrame(small_data, [\"id\", \"department\"])\n",
        "\n",
        "# Применение broadcast к маленькому DataFrame\n",
        "broadcast_small_df = broadcast(small_df)\n",
        "\n",
        "# Выполнение join операции\n",
        "joined_df = large_df.join(broadcast_small_df, \"id\")\n",
        "\n",
        "\n",
        "joined_df.show()\n",
        "\n",
        "\n",
        "spark.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Мы создаем большой DataFrame large_df, который содержит миллион записей.\n",
        "- Мы создаем маленький DataFrame small_df, который содержит несколько записей.\n",
        "- Мы используем функцию broadcast для маленького DataFrame. Он распространит копию маленького DataFrame на все узлы кластера.\n",
        "- Мы выполняем join между большим DataFrame и broadcast-версией маленького DataFrame. Благодаря broadcast переменной, join выполняется локально на каждом узле без необходимости shuffle операций.\n"
      ],
      "metadata": {
        "id": "yS29RH7EgSIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "И, на всякий случай, сравним его с cache и persist.\n",
        "\n"
      ],
      "metadata": {
        "id": "aMvZWGWUgZIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Broadcast**: Распространяет неизменяемые данные по всему кластеру для эффективного использования.\n",
        "- **Cache/Persist**: Кэширует промежуточные результаты вычислений для многократного использования.\n",
        "- **Broadcast**: Используется для небольших объектов, которые должны быть доступны на всех узлах.\n",
        "- **Cache/Persist**: Используется для больших наборов данных, которые обрабатываются многократно.\n",
        "- **Broadcast**: Отправляет копию данных на каждый узел, чтобы избежать многократной передачи.\n",
        "- **Cache/Persist**: Хранит данные в памяти или на диске для последующего использования без повторных вычислений.\n"
      ],
      "metadata": {
        "id": "z6sE1hv0gZr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оптимизировать данный код (запуск как будто на кластере)"
      ],
      "metadata": {
        "id": "JTV9fwwggyuT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Broadcast Join Example\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Пример большого DataFrame\n",
        "large_data = [(i, f\"Name_{i % 5}\") for i in range(1000000)]\n",
        "large_df = spark.createDataFrame(large_data, [\"id\", \"name\"])\n",
        "\n",
        "# Пример маленького DataFrame\n",
        "small_data = [(1, \"HR\"), (2, \"Finance\"), (3, \"Engineering\")]\n",
        "small_df = spark.createDataFrame(small_data, [\"id\", \"department\"])\n",
        "\n",
        "# Применение broadcast к маленькому DataFrame\n",
        "broadcast_small_df = broadcast(small_df)\n",
        "\n",
        "# Выполнение join операции\n",
        "joined_df = large_df.join(broadcast_small_df, \"id\")\n",
        "\n",
        "\n",
        "joined_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MSrOqC61g85H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "МОИ идеи что сделать:\n",
        "1. Catalyst Optimizer и Tungsten Execution Engine\n",
        "2. Разделить большой дф на бакеты для более эффективного соединения"
      ],
      "metadata": {
        "id": "7GNirzVDg-6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Решение:"
      ],
      "metadata": {
        "id": "6sBAMyaNiqQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если ваш большой DataFrame не равномерно партиционирован, это может привести к дисбалансу нагрузки. Перераспределение данных с использованием метода repartition может помочь.\n",
        "\n"
      ],
      "metadata": {
        "id": "h1sRA9O5in1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Перераспределение большого DataFrame на большее количество партиций для улучшения параллелизма\n",
        "large_df = large_df.repartition(200)  # Выберите разумное количество партиций в зависимости от вашего кластера\n"
      ],
      "metadata": {
        "id": "95g1ygU2inhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если большой DataFrame используется в нескольких местах, его кэширование может улучшить производительность.\n",
        "\n"
      ],
      "metadata": {
        "id": "SbqoHW8jixkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Кэширование большого DataFrame\n",
        "large_df.cache()\n"
      ],
      "metadata": {
        "id": "xtcV837diyDc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тюнинг конфигурации Spark.\n",
        "\n"
      ],
      "metadata": {
        "id": "owaYAQBQiz4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  # Количество партиций для операций shuffle\n",
        "spark.conf.set(\"spark.executor.memory\", \"4g\")  # Размер памяти для каждого исполнителя\n",
        "spark.conf.set(\"spark.driver.memory\", \"4g\")  # Размер памяти для драйвера\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "gxAy0eumi08d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В целом идеи были в правильном направлении -\n",
        " - переразделить данные для равномерности\n",
        " - про кеш не подумал (многократное использование вроде не подразумевалось, а так конечно дада)\n",
        " - Ну и размер памяти для исполнителей и драйвера (SparkSession) весьма большой (на кластере ж все-таки), об этом не подумал."
      ],
      "metadata": {
        "id": "J51i-baji8S0"
      }
    }
  ]
}