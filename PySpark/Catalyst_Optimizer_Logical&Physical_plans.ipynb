{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkaFQrc6yLSFHJeV3/ThEv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyaDenisov88/dataenj/blob/main/PySpark/Catalyst_Optimizer_Logical%26Physical_plans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZiJQMT70Xtw",
        "outputId": "dca0e128-bd66-4250-cce0-d7a0b3a6a4b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=a5318b7fcd4e2e8595172956ab51c7fb6a6fd411d4a417289993a91f55f28569\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark"
      ],
      "metadata": {
        "id": "jjZvIutm0Lw_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустим код и увидим огромную простыню логов. Стоит отметить, что все планы запросов читаются сверху вниз.\n",
        "\n"
      ],
      "metadata": {
        "id": "IAruvpYX0_WG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSEdFd_T0Da-",
        "outputId": "af22bf41-3782-4980-f3d4-cb6f2bd41705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Parsed Logical Plan ==\n",
            "'Sort ['Name ASC NULLS FIRST], true\n",
            "+- Project [Name#0, Age#1L]\n",
            "   +- Filter (Age#1L > cast(30 as bigint))\n",
            "      +- LogicalRDD [Name#0, Age#1L], false\n",
            "\n",
            "== Analyzed Logical Plan ==\n",
            "Name: string, Age: bigint\n",
            "Sort [Name#0 ASC NULLS FIRST], true\n",
            "+- Project [Name#0, Age#1L]\n",
            "   +- Filter (Age#1L > cast(30 as bigint))\n",
            "      +- LogicalRDD [Name#0, Age#1L], false\n",
            "\n",
            "== Optimized Logical Plan ==\n",
            "Sort [Name#0 ASC NULLS FIRST], true\n",
            "+- Filter (isnotnull(Age#1L) AND (Age#1L > 30))\n",
            "   +- LogicalRDD [Name#0, Age#1L], false\n",
            "\n",
            "== Physical Plan ==\n",
            "AdaptiveSparkPlan isFinalPlan=false\n",
            "+- Sort [Name#0 ASC NULLS FIRST], true, 0\n",
            "   +- Exchange rangepartitioning(Name#0 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=12]\n",
            "      +- Filter (isnotnull(Age#1L) AND (Age#1L > 30))\n",
            "         +- Scan ExistingRDD[Name#0,Age#1L]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Catalyst Optimizer Example\").getOrCreate()\n",
        "\n",
        "\n",
        "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "\n",
        "result = df.filter(df.Age > 30).select(\"Name\", \"Age\").orderBy(\"Name\")\n",
        "\n",
        "# Показываем логический и физический планы\n",
        "result.explain(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Начнем с первого абзаца. **Parsed Logical Plan** показывает первосозданный, неоптимизированный план выполнения запроса. Важно, что *он пока что не смотрит на типы данных*.\n",
        "\n",
        "- Sort ['Name ASC NULLS FIRST], true: Сортировка по столбцу Name по возрастанию с NULL значениями первыми.\n",
        "- Project ['Name, 'Age]: Выбор столбцов Name и Age.\n",
        "- Filter ('Age > 30): Фильтрация строк, где Age больше 30."
      ],
      "metadata": {
        "id": "zhQZ06k21XqE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Двигаемся дальше. Далее у нас **Analyzed Logical Plan**. На этом этапе Catalyst анализирует и проверяет логический план, добавляя *информацию о типах данных и идентификаторах столбцов*. Заметим также, что *появились ссылки на столбцы*.\n",
        "\n",
        "- Name: string, Age: int: Информация о типах данных для столбцов.\n",
        "- Sort [Name#2 ASC NULLS FIRST], true: Сортировка по столбцу Name (идентификатор столбца Name#2).\n",
        "- Project [Name#2, Age#3]: Выбор столбцов Name (идентификатор Name#2) и Age (идентификатор Age#3).\n",
        "- Filter (Age#3 > 30): Фильтрация строк, где Age (идентификатор Age#3) больше 30."
      ],
      "metadata": {
        "id": "Z08xiEvj1ent"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Двигаемся еще ниже. Это уже **Optimized Logical Plan**. Видим, что оптимизаций никаких не произошло, ведь запрос заранее составлен правильно.\n",
        "\n"
      ],
      "metadata": {
        "id": "1EgOd_ON2vQY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "И, наконец, посмотрим в конец простыни логов и увидим **Physical Plan**. Он достаточно прост."
      ],
      "metadata": {
        "id": "zav7tCZ92xTW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2s4c2VTM0bgo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cey_7xyI0bjx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBl5yrRF0bmN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dxncPcpK0bo1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VW-XM8sT0brU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}