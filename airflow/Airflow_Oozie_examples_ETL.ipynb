{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPoht/dSQQgSelQ2c40o2ZS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyaDenisov88/dataenj/blob/main/airflow/Airflow_Oozie_examples_ETL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ETL процесс в Oozie определяется с помощью XML-файлов, которые описывают последовательность выполнения задач. Наш пример из workflow.xml (помимо него необходимы еще файлы, но в контесте нашего рассказа \"хватит\" и его):\n",
        "\n"
      ],
      "metadata": {
        "id": "ARmadI5U7MWl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NVSvR5a7EJk"
      },
      "outputs": [],
      "source": [
        "<workflow-app name=\"etl-workflow\" xmlns=\"uri:oozie:workflow:0.5\">\n",
        "    <start to=\"extract-node\" />\n",
        "\n",
        "    <action name=\"extract-node\">\n",
        "        <shell>\n",
        "            <job-tracker>${jobTracker}</job-tracker>\n",
        "            <name-node>${nameNode}</name-node>\n",
        "            <exec>${shellScriptExtract}</exec>\n",
        "            <file>${shellScriptExtract}#extract.sh</file>\n",
        "        </shell>\n",
        "        <ok to=\"transform-node\" />\n",
        "        <error to=\"fail\" />\n",
        "    </action>\n",
        "\n",
        "    <action name=\"transform-node\">\n",
        "        <shell>\n",
        "            <job-tracker>${jobTracker}</job-tracker>\n",
        "            <name-node>${nameNode}</name-node>\n",
        "            <exec>${shellScriptTransform}</exec>\n",
        "            <file>${shellScriptTransform}#transform.sh</file>\n",
        "        </shell>\n",
        "        <ok to=\"load-node\" />\n",
        "        <error to=\"fail\" />\n",
        "    </action>\n",
        "\n",
        "    <action name=\"load-node\">\n",
        "        <shell>\n",
        "            <job-tracker>${jobTracker}</job-tracker>\n",
        "            <name-node>${nameNode}</name-node>\n",
        "            <exec>${shellScriptLoad}</exec>\n",
        "            <file>${shellScriptLoad}#load.sh</file>\n",
        "        </shell>\n",
        "        <ok to=\"end\" />\n",
        "        <error to=\"fail\" />\n",
        "    </action>\n",
        "\n",
        "    <kill name=\"fail\">\n",
        "        <message>ETL process failed</message>\n",
        "    </kill>\n",
        "\n",
        "    <end name=\"end\" />\n",
        "</workflow-app>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "И теперь посмотрим как это выглядит с использованием Airflow. ETL процесс в Airflow определяется с помощью Python-кода, который описывает DAG (Directed Acyclic Graph) - тот самый направленный ациклический граф.Тут файл будет называться условно etl_dag.py  и вот его содержимое:\n",
        "\n"
      ],
      "metadata": {
        "id": "eQ-Q2myY7Rl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "from airflow import DAG\n",
        "from airflow.operators.bash_operator import BashOperator\n",
        "\n",
        "default_args = {\n",
        "    'owner': 'airflow',\n",
        "    'depends_on_past': False,\n",
        "    'start_date': datetime(2024, 7, 1),\n",
        "    'email_on_failure': False,\n",
        "    'email_on_retry': False,\n",
        "    'retries': 1,\n",
        "    'retry_delay': timedelta(minutes=5),\n",
        "}\n",
        "\n",
        "dag = DAG(\n",
        "    'etl_workflow',\n",
        "    default_args=default_args,\n",
        "    description='Example ETL workflow for DE students',\n",
        "    schedule_interval=timedelta(days=1),\n",
        ")\n",
        "\n",
        "extract_task = BashOperator(\n",
        "    task_id='extract',\n",
        "    bash_command='sh /path/to/extract.sh',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "transform_task = BashOperator(\n",
        "    task_id='transform',\n",
        "    bash_command='sh /path/to/transform.sh',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "load_task = BashOperator(\n",
        "    task_id='load',\n",
        "    bash_command='sh /path/to/load.sh',\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "extract_task >> transform_task >> load_task\n"
      ],
      "metadata": {
        "id": "9nV4Faxr7Uwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Просто сравните \"визуальную\" составляющую. Очевидно, что Airflow даже на этапе описания тех или иных процессов выглядит удобнее, понятнее и аккуратнее. Не говоря уже об интерфейсе).\n",
        "\n",
        "Apache Airflow предоставляет большую гибкость и удобство использования по сравнению с Apache Oozie, особенно для разработчиков, которые предпочитают использовать Python. В то время как Oozie все еще полезен для интеграции с экосистемой Hadoop, Airflow выигрывает в плане масштабируемости, расширяемости и поддержки сообщества.\n",
        "\n"
      ],
      "metadata": {
        "id": "78JGOlcY7ajM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSm8AWeS7Z4w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}